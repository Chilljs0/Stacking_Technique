{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Stacking?\n",
    "\n",
    "Stacking is a technique where multiple Base Models are trained on the same dataset. After this, the predictions of these models are used as the input features of a seperate model, called the Meta Model. This is similar to other techniques such as Bagging and Boosting but stacking can use any models where as the Bagged and Boosted models are more specific on their base model needs.\n",
    "\n",
    "Stacking is advantageous when you need to leverage different models with different strengths to tackle multiple problems in one model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Stacking Works\n",
    "\n",
    "1. Each base model, in this instance we have Random Forest, Gradient Boosting, and a Decision Tree, are trained on the full dataset. Like any other predictive model you run, it will make predictions on the data set.\n",
    "\n",
    "2. A new dataset is created with the predictions of the base models. This is called the meta data set and is used to train the meta model. Each column is the predictions of a base model.\n",
    "\n",
    "3. The new meta model is typically a simple model like Linear/Logistic Regression. This is used to learn the relationships and patterns between the predictions of the base models. It helps pinpoint where a base model is successful in predictions and where its weaknesses are. It puts certain weights on these areas.\n",
    "\n",
    "4. The meta model then makes predictions on the original training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "The dataset is loaded from a CSV file.\n",
    "\n",
    "The plan is to use tree-based models. Since there are little preprocessing steps requiredf or tree-based models and it can robustly handle class imbalance, the processing is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"Base.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical columns are compiled here and given numerical labels from LabelEncoder. This is necessary for the models since they require numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalanace with Resampling\n",
    "\n",
    "The dataset is extremely imbalanced, even for tree-based models. Fraud cases are around 10,000 which is only 1% of the non-fraud cases around 1,000,000. This will skew the accuracy measurements of the model. Even if we predict all cases to be non-fraud we will get a 99% accuracy rating. While the number is good, we accomplish nothing since we are trying to be able to predict fraud cases. \n",
    "\n",
    "Here undersampling is used to help class imbalance. We reduce to the majority case from 1,000,000 to 100,000 so that the results are more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = df[df['fraud_bool'] == 0]\n",
    "df_minority = df[df['fraud_bool'] == 1]\n",
    "\n",
    "df_majority_undersampled = resample(df_majority, \n",
    "                                    replace=False,       \n",
    "                                    n_samples=100000,\n",
    "                                    random_state=42)\n",
    "\n",
    "df_undersampled = pd.concat([df_majority_undersampled, df_minority])\n",
    "\n",
    "y = df_undersampled['fraud_bool']\n",
    "x = df_undersampled.drop('fraud_bool', axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "\n",
    "After the processing of data, the data is split into a 60-40 split. This allows us to train the model on 60% of the data and test and validate the model on 40$ of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation\n",
    "\n",
    "Three models are trained, wach with its own report and confusion matrix for evaluation. The metrics are saved for a comparison table at the end.\n",
    "\n",
    "The metrics used to evaluate are Accuracy, Precision, Recall, and F1 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "model_metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "First, is Random Forest. This is an ensemble(a group of items viewed as a whole) model that uses multiple decision trees to improve accuracy and overfitting. The number of trees in this example is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95     40070\n",
      "           1       0.70      0.18      0.29      4342\n",
      "\n",
      "    accuracy                           0.91     44412\n",
      "   macro avg       0.81      0.59      0.62     44412\n",
      "weighted avg       0.90      0.91      0.89     44412\n",
      "\n",
      "Random Forest Confusion Matrix:\n",
      " [[39725   345]\n",
      " [ 3539   803]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf_model.fit(x_train, y_train)\n",
    "y_pred_rf = rf_model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "report = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "\n",
    "model_metrics.append({\n",
    "    \"Model\": \"Random Forest\", \n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision (Weighted Avg)\": report['weighted avg']['precision'],\n",
    "    \"Recall (Weighted Avg)\": report['weighted avg']['recall'],\n",
    "    \"F1-score (Weighted Avg)\": report['weighted avg']['f1-score']\n",
    "})\n",
    "\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Random Forest Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Second, is Gradient Boosing. This is a boosting algorith that builds trees sequentially to correct the errors of the previous tree. Like Random Forest, this helps improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96     40070\n",
      "           1       0.69      0.32      0.43      4342\n",
      "\n",
      "    accuracy                           0.92     44412\n",
      "   macro avg       0.81      0.65      0.70     44412\n",
      "weighted avg       0.91      0.92      0.91     44412\n",
      "\n",
      "Gradient Boosting Confusion Matrix:\n",
      " [[39464   606]\n",
      " [ 2973  1369]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(x_train, y_train)\n",
    "y_pred_gb = gb_model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "report = classification_report(y_test, y_pred_gb, output_dict=True)\n",
    "\n",
    "model_metrics.append({\n",
    "    \"Model\": \"Gradient Boosting\", \n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision (Weighted Avg)\": report['weighted avg']['precision'],\n",
    "    \"Recall (Weighted Avg)\": report['weighted avg']['recall'],\n",
    "    \"F1-score (Weighted Avg)\": report['weighted avg']['f1-score']\n",
    "})\n",
    "\n",
    "print(\"Gradient Boosting Classification Report:\\n\", classification_report(y_test, y_pred_gb))\n",
    "print(\"Gradient Boosting Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Last, is a Decision Tree. This is one simple tree that splits data based on values of the features. This, like many basic models, is useful for baseline estimates for other, stronger methods, like the ones above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     40070\n",
      "           1       0.33      0.34      0.33      4342\n",
      "\n",
      "    accuracy                           0.87     44412\n",
      "   macro avg       0.63      0.63      0.63     44412\n",
      "weighted avg       0.87      0.87      0.87     44412\n",
      "\n",
      "Decision Tree Confusion Matrix:\n",
      " [[37071  2999]\n",
      " [ 2877  1465]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "dt_model.fit(x_train, y_train)\n",
    "y_pred_dt = dt_model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "report = classification_report(y_test, y_pred_dt, output_dict=True)\n",
    "\n",
    "model_metrics.append({\n",
    "    \"Model\": \"Decision Tree\", \n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision (Weighted Avg)\": report['weighted avg']['precision'],\n",
    "    \"Recall (Weighted Avg)\": report['weighted avg']['recall'],\n",
    "    \"F1-score (Weighted Avg)\": report['weighted avg']['f1-score']\n",
    "})\n",
    "\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
    "print(\"Decision Tree Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Models\n",
    "\n",
    "Here, stacking is used. Stacking is a technique that combines multiple models to improve predictive performance. Like Random Forest with tree, stacking does it with models. In this example the same three models are used as above to stack on top of each other to improve accuracy.\n",
    "\n",
    "In stacking we have:\n",
    "\n",
    "Base models: Here Random Forest, Gradient Boosting, and a Decision Tree is used. They independently of each other learn patterns in the data to predict.\n",
    "\n",
    "Meta Model: In this example, a Logistic Regression model is used as the final model to take predictions of base models as inputs and learn how best to combine them to improve the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model with Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96     40070\n",
      "           1       0.66      0.35      0.45      4342\n",
      "\n",
      "    accuracy                           0.92     44412\n",
      "   macro avg       0.80      0.66      0.70     44412\n",
      "weighted avg       0.91      0.92      0.91     44412\n",
      "\n",
      "Stacking Model with Logistic Regression Confusion Matrix:\n",
      " [[39297   773]\n",
      " [ 2843  1499]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42))\n",
    "]\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegression(random_state=42)\n",
    ")\n",
    "\n",
    "stacking_model.fit(x_train, y_train)\n",
    "y_pred_stacking = stacking_model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "report = classification_report(y_test, y_pred_stacking, output_dict=True)\n",
    "\n",
    "model_metrics.append({\n",
    "    \"Model\": \"Stacking Model\", \n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision (Weighted Avg)\": report['weighted avg']['precision'],\n",
    "    \"Recall (Weighted Avg)\": report['weighted avg']['recall'],\n",
    "    \"F1-score (Weighted Avg)\": report['weighted avg']['f1-score']\n",
    "})\n",
    "\n",
    "print(\"Stacking Model with Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_stacking))\n",
    "print(\"Stacking Model with Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_stacking))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Here are the final statistics and ranks of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  Accuracy  Precision (Weighted Avg)  \\\n",
      "0      Random Forest  0.912546                  0.896816   \n",
      "1  Gradient Boosting  0.919414                  0.906794   \n",
      "2      Decision Tree  0.867693                  0.869341   \n",
      "3     Stacking Model  0.918581                  0.905867   \n",
      "\n",
      "   Recall (Weighted Avg)  F1-score (Weighted Avg)  Accuracy Rank  \\\n",
      "0               0.912546                 0.888782            3.0   \n",
      "1               0.919414                 0.905472            1.0   \n",
      "2               0.867693                 0.868509            4.0   \n",
      "3               0.918581                 0.906865            2.0   \n",
      "\n",
      "   Precision (Weighted Avg) Rank  Recall (Weighted Avg) Rank  \\\n",
      "0                            3.0                         3.0   \n",
      "1                            1.0                         1.0   \n",
      "2                            4.0                         4.0   \n",
      "3                            2.0                         2.0   \n",
      "\n",
      "   F1-score (Weighted Avg) Rank  \n",
      "0                           3.0  \n",
      "1                           2.0  \n",
      "2                           4.0  \n",
      "3                           1.0  \n"
     ]
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame(model_metrics)\n",
    "\n",
    "for metric in [\"Accuracy\", \"Precision (Weighted Avg)\", \"Recall (Weighted Avg)\", \"F1-score (Weighted Avg)\"]:\n",
    "    metrics_df[f\"{metric} Rank\"] = metrics_df[metric].rank(ascending=False, method='min')\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Based on the ranks of the models, it seems that Gradient Boosting actually performed slightly better than the Stacking Model. It scored rank 1 on everything except F1-Score but only by a miniscule margin. Infact most of the models perform quite well. \n",
    "\n",
    "Taking into account the complexity of each model it would seem that Stacking is not neccesary in this particular case and maybe require a better base model selection, especially since Decision Trees are obsolete as major predictive models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
